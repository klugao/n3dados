{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Modelagem e Avalia√ß√£o Comparativa\n",
        "## Sistema de Predi√ß√£o de Evas√£o Estudantil\n",
        "\n",
        "Este notebook implementa tr√™s modelos de classifica√ß√£o, avalia seu desempenho e compara os resultados para selecionar o melhor modelo.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Importa√ß√µes\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "import warnings\n",
        "warnings.filterwarnings('ignore')\n",
        "\n",
        "# Machine Learning\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.preprocessing import StandardScaler, LabelEncoder\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "from sklearn.neighbors import KNeighborsClassifier\n",
        "\n",
        "# M√©tricas de avalia√ß√£o\n",
        "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score\n",
        "from sklearn.metrics import classification_report, confusion_matrix\n",
        "from sklearn.metrics import roc_curve, auc, roc_auc_score\n",
        "\n",
        "# Serializa√ß√£o\n",
        "import joblib\n",
        "\n",
        "# Configura√ß√µes\n",
        "plt.style.use('seaborn-v0_8')\n",
        "sns.set_palette(\"husl\")\n",
        "plt.rcParams['figure.figsize'] = (12, 6)\n",
        "\n",
        "%matplotlib inline\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 1. Carregamento e Prepara√ß√£o dos Dados\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Carregar dataset\n",
        "df = pd.read_csv('../data/student_dropout_dataset.csv')\n",
        "\n",
        "print(f\"üìä Dataset carregado: {df.shape}\")\n",
        "print(f\"üìã Colunas: {list(df.columns)}\")\n",
        "df.head()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Separar features e target\n",
        "X = df.drop(['dropout', 'student_id'], axis=1)\n",
        "y = df['dropout']\n",
        "\n",
        "print(f\"üìä Features (X): {X.shape}\")\n",
        "print(f\"üìä Target (y): {y.shape}\")\n",
        "print(f\"üìà Distribui√ß√£o do target:\\n{y.value_counts()}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Codificar vari√°veis categ√≥ricas\n",
        "label_encoders = {}\n",
        "for col in X.select_dtypes(include=['object']).columns:\n",
        "    le = LabelEncoder()\n",
        "    X[col] = le.fit_transform(X[col])\n",
        "    label_encoders[col] = le\n",
        "    print(f\"‚úÖ Codificada: {col}\")\n",
        "\n",
        "print(f\"\\nüìä Features ap√≥s encoding: {X.shape}\")\n",
        "print(f\"üìã Tipos de dados:\\n{X.dtypes}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Dividir em treino e teste\n",
        "X_train, X_test, y_train, y_test = train_test_split(\n",
        "    X, y, test_size=0.2, random_state=42, stratify=y\n",
        ")\n",
        "\n",
        "print(f\"üìä Conjunto de Treino: {X_train.shape[0]} amostras\")\n",
        "print(f\"üìä Conjunto de Teste: {X_test.shape[0]} amostras\")\n",
        "print(f\"\\nüìà Distribui√ß√£o no treino:\\n{y_train.value_counts()}\")\n",
        "print(f\"\\nüìà Distribui√ß√£o no teste:\\n{y_test.value_counts()}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Normalizar features (importante para Regress√£o Log√≠stica e KNN)\n",
        "scaler = StandardScaler()\n",
        "X_train_scaled = scaler.fit_transform(X_train)\n",
        "X_test_scaled = scaler.transform(X_test)\n",
        "\n",
        "print(\"‚úÖ Features normalizadas com StandardScaler\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 2. Explica√ß√£o das M√©tricas de Avalia√ß√£o\n",
        "\n",
        "Antes de treinar os modelos, vamos entender o que cada m√©trica significa e por que √© relevante para nosso problema de predi√ß√£o de evas√£o:\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### 2.1 Acur√°cia (Accuracy)\n",
        "- **O que mede**: Taxa de acertos gerais do modelo\n",
        "- **F√≥rmula**: (Verdadeiros Positivos + Verdadeiros Negativos) / Total\n",
        "- **Relev√¢ncia**: D√° uma vis√£o geral do desempenho, mas pode ser enganosa em datasets desbalanceados\n",
        "\n",
        "### 2.2 Precis√£o (Precision)\n",
        "- **O que mede**: Entre os estudantes preditos como evas√£o, quantos realmente evadiram\n",
        "- **F√≥rmula**: Verdadeiros Positivos / (Verdadeiros Positivos + Falsos Positivos)\n",
        "- **Relev√¢ncia**: Importante para evitar alarmes falsos. Queremos ter certeza quando identificamos um estudante em risco.\n",
        "\n",
        "### 2.3 Recall (Sensibilidade)\n",
        "- **O que mede**: Entre os estudantes que realmente evadiram, quantos foram identificados pelo modelo\n",
        "- **F√≥rmula**: Verdadeiros Positivos / (Verdadeiros Positivos + Falsos Negativos)\n",
        "- **Relev√¢ncia**: **CRUCIAL** para nosso problema! N√£o podemos deixar passar estudantes em risco de evas√£o.\n",
        "\n",
        "### 2.4 F1-Score\n",
        "- **O que mede**: M√©dia harm√¥nica entre Precis√£o e Recall\n",
        "- **F√≥rmula**: 2 √ó (Precis√£o √ó Recall) / (Precis√£o + Recall)\n",
        "- **Relev√¢ncia**: Balanceia Precis√£o e Recall, √∫til quando precisamos de um equil√≠brio entre ambos.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 3. Treinamento dos Modelos\n",
        "\n",
        "Vamos treinar tr√™s algoritmos diferentes:\n",
        "1. **Regress√£o Log√≠stica** - Modelo linear interpret√°vel\n",
        "2. **Random Forest** - Ensemble robusto com √°rvores de decis√£o\n",
        "3. **KNN (K-Nearest Neighbors)** - M√©todo n√£o-param√©trico baseado em proximidade\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Dicion√°rio para armazenar os modelos\n",
        "models = {}\n",
        "\n",
        "# 1. Regress√£o Log√≠stica\n",
        "print(\"üîÑ Treinando Regress√£o Log√≠stica...\")\n",
        "lr_model = LogisticRegression(random_state=42, max_iter=1000)\n",
        "lr_model.fit(X_train_scaled, y_train)\n",
        "models['Regress√£o Log√≠stica'] = lr_model\n",
        "print(\"‚úÖ Regress√£o Log√≠stica treinada!\")\n",
        "\n",
        "# 2. Random Forest\n",
        "print(\"\\nüîÑ Treinando Random Forest...\")\n",
        "rf_model = RandomForestClassifier(n_estimators=100, random_state=42, n_jobs=-1)\n",
        "rf_model.fit(X_train, y_train)\n",
        "models['Random Forest'] = rf_model\n",
        "print(\"‚úÖ Random Forest treinado!\")\n",
        "\n",
        "# 3. KNN\n",
        "print(\"\\nüîÑ Treinando KNN...\")\n",
        "knn_model = KNeighborsClassifier(n_neighbors=5)\n",
        "knn_model.fit(X_train_scaled, y_train)\n",
        "models['KNN'] = knn_model\n",
        "print(\"‚úÖ KNN treinado!\")\n",
        "\n",
        "print(f\"\\n‚úÖ Total de modelos treinados: {len(models)}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 4. Avalia√ß√£o dos Modelos\n",
        "\n",
        "Vamos avaliar cada modelo usando as quatro m√©tricas definidas.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Fun√ß√£o para avaliar modelo\n",
        "def evaluate_model(model, X_test_data, y_test_data, model_name, use_scaled=True):\n",
        "    \"\"\"Avalia um modelo e retorna as m√©tricas\"\"\"\n",
        "    if use_scaled:\n",
        "        y_pred = model.predict(X_test_scaled)\n",
        "        y_pred_proba = model.predict_proba(X_test_scaled)[:, 1]\n",
        "    else:\n",
        "        y_pred = model.predict(X_test_data)\n",
        "        y_pred_proba = model.predict_proba(X_test_data)[:, 1]\n",
        "    \n",
        "    metrics = {\n",
        "        'Modelo': model_name,\n",
        "        'Acur√°cia': accuracy_score(y_test_data, y_pred),\n",
        "        'Precis√£o': precision_score(y_test_data, y_pred),\n",
        "        'Recall': recall_score(y_test_data, y_pred),\n",
        "        'F1-Score': f1_score(y_test_data, y_pred),\n",
        "        'ROC-AUC': roc_auc_score(y_test_data, y_pred_proba)\n",
        "    }\n",
        "    \n",
        "    return metrics, y_pred, y_pred_proba\n",
        "\n",
        "# Avaliar todos os modelos\n",
        "results = []\n",
        "predictions = {}\n",
        "\n",
        "print(\"üìä Avaliando Modelos:\\n\")\n",
        "\n",
        "# Regress√£o Log√≠stica\n",
        "metrics_lr, pred_lr, proba_lr = evaluate_model(\n",
        "    lr_model, X_test_scaled, y_test, 'Regress√£o Log√≠stica', use_scaled=True\n",
        ")\n",
        "results.append(metrics_lr)\n",
        "predictions['Regress√£o Log√≠stica'] = {'pred': pred_lr, 'proba': proba_lr}\n",
        "print(f\"‚úÖ {metrics_lr['Modelo']} avaliado!\")\n",
        "\n",
        "# Random Forest\n",
        "metrics_rf, pred_rf, proba_rf = evaluate_model(\n",
        "    rf_model, X_test, y_test, 'Random Forest', use_scaled=False\n",
        ")\n",
        "results.append(metrics_rf)\n",
        "predictions['Random Forest'] = {'pred': pred_rf, 'proba': proba_rf}\n",
        "print(f\"‚úÖ {metrics_rf['Modelo']} avaliado!\")\n",
        "\n",
        "# KNN\n",
        "metrics_knn, pred_knn, proba_knn = evaluate_model(\n",
        "    knn_model, X_test_scaled, y_test, 'KNN', use_scaled=True\n",
        ")\n",
        "results.append(metrics_knn)\n",
        "predictions['KNN'] = {'pred': pred_knn, 'proba': proba_knn}\n",
        "print(f\"‚úÖ {metrics_knn['Modelo']} avaliado!\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Criar DataFrame com resultados\n",
        "results_df = pd.DataFrame(results)\n",
        "results_df = results_df.set_index('Modelo')\n",
        "\n",
        "# Formatar para melhor visualiza√ß√£o\n",
        "results_display = results_df.copy()\n",
        "for col in ['Acur√°cia', 'Precis√£o', 'Recall', 'F1-Score', 'ROC-AUC']:\n",
        "    results_display[col] = results_display[col].apply(lambda x: f\"{x:.4f}\")\n",
        "\n",
        "print(\"üìä RESULTADOS COMPARATIVOS DOS MODELOS:\\n\")\n",
        "print(results_display.to_string())\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Visualiza√ß√£o comparativa\n",
        "fig, axes = plt.subplots(2, 2, figsize=(15, 12))\n",
        "\n",
        "metrics_to_plot = ['Acur√°cia', 'Precis√£o', 'Recall', 'F1-Score']\n",
        "for idx, metric in enumerate(metrics_to_plot):\n",
        "    ax = axes[idx // 2, idx % 2]\n",
        "    results_df[metric].plot(kind='barh', ax=ax, color=['#3498db', '#2ecc71', '#e74c3c'])\n",
        "    ax.set_title(f'{metric}', fontsize=14, fontweight='bold')\n",
        "    ax.set_xlabel('Score', fontsize=12)\n",
        "    ax.set_ylabel('')\n",
        "    ax.set_xlim(0, 1)\n",
        "    for i, v in enumerate(results_df[metric]):\n",
        "        ax.text(v + 0.01, i, f'{v:.3f}', va='center', fontweight='bold')\n",
        "\n",
        "plt.suptitle('Compara√ß√£o de M√©tricas entre Modelos', fontsize=16, fontweight='bold', y=0.995)\n",
        "plt.tight_layout()\n",
        "plt.show()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 5. Matrizes de Confus√£o\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Visualizar matrizes de confus√£o\n",
        "fig, axes = plt.subplots(1, 3, figsize=(18, 5))\n",
        "\n",
        "model_names = ['Regress√£o Log√≠stica', 'Random Forest', 'KNN']\n",
        "for idx, model_name in enumerate(model_names):\n",
        "    cm = confusion_matrix(y_test, predictions[model_name]['pred'])\n",
        "    sns.heatmap(cm, annot=True, fmt='d', cmap='Blues', ax=axes[idx],\n",
        "                xticklabels=['N√£o Evadiu', 'Evadiu'],\n",
        "                yticklabels=['N√£o Evadiu', 'Evadiu'])\n",
        "    axes[idx].set_title(f'{model_name}', fontsize=14, fontweight='bold')\n",
        "    axes[idx].set_ylabel('Verdadeiro', fontsize=12)\n",
        "    axes[idx].set_xlabel('Predito', fontsize=12)\n",
        "\n",
        "plt.suptitle('Matrizes de Confus√£o', fontsize=16, fontweight='bold', y=1.02)\n",
        "plt.tight_layout()\n",
        "plt.show()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 6. Curvas ROC\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Plotar curvas ROC\n",
        "plt.figure(figsize=(10, 8))\n",
        "\n",
        "for model_name in model_names:\n",
        "    fpr, tpr, _ = roc_curve(y_test, predictions[model_name]['proba'])\n",
        "    roc_auc = auc(fpr, tpr)\n",
        "    plt.plot(fpr, tpr, label=f'{model_name} (AUC = {roc_auc:.3f})', linewidth=2)\n",
        "\n",
        "plt.plot([0, 1], [0, 1], 'k--', label='Classificador Aleat√≥rio', linewidth=1)\n",
        "plt.xlim([0.0, 1.0])\n",
        "plt.ylim([0.0, 1.05])\n",
        "plt.xlabel('Taxa de Falsos Positivos', fontsize=12)\n",
        "plt.ylabel('Taxa de Verdadeiros Positivos', fontsize=12)\n",
        "plt.title('Curvas ROC - Compara√ß√£o de Modelos', fontsize=16, fontweight='bold')\n",
        "plt.legend(loc=\"lower right\", fontsize=11)\n",
        "plt.grid(True, alpha=0.3)\n",
        "plt.tight_layout()\n",
        "plt.show()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 7. An√°lise Comparativa e Sele√ß√£o do Melhor Modelo\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# An√°lise detalhada\n",
        "print(\"=\" * 70)\n",
        "print(\"AN√ÅLISE COMPARATIVA DOS MODELOS\")\n",
        "print(\"=\" * 70)\n",
        "\n",
        "# Identificar melhor modelo em cada m√©trica\n",
        "for metric in ['Acur√°cia', 'Precis√£o', 'Recall', 'F1-Score', 'ROC-AUC']:\n",
        "    best_model = results_df[metric].idxmax()\n",
        "    best_score = results_df[metric].max()\n",
        "    print(f\"\\nüèÜ Melhor em {metric}: {best_model} ({best_score:.4f})\")\n",
        "\n",
        "# Selecionar melhor modelo baseado em F1-Score (balanceia Precis√£o e Recall)\n",
        "best_model_name = results_df['F1-Score'].idxmax()\n",
        "best_model_score = results_df['F1-Score'].max()\n",
        "\n",
        "print(\"\\n\" + \"=\" * 70)\n",
        "print(f\"üéØ MELHOR MODELO SELECIONADO: {best_model_name}\")\n",
        "print(f\"   F1-Score: {best_model_score:.4f}\")\n",
        "print(\"=\" * 70)\n",
        "\n",
        "# Mostrar todas as m√©tricas do melhor modelo\n",
        "print(f\"\\nüìä M√©tricas Completas do {best_model_name}:\")\n",
        "print(results_df.loc[best_model_name].to_string())\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 8. Salvamento do Melhor Modelo\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Selecionar o melhor modelo\n",
        "if best_model_name == 'Regress√£o Log√≠stica':\n",
        "    best_model = lr_model\n",
        "    model_uses_scaled = True\n",
        "elif best_model_name == 'Random Forest':\n",
        "    best_model = rf_model\n",
        "    model_uses_scaled = False\n",
        "else:  # KNN\n",
        "    best_model = knn_model\n",
        "    model_uses_scaled = True\n",
        "\n",
        "# Salvar modelo\n",
        "model_path = '../modelo_final.pkl'\n",
        "joblib.dump(best_model, model_path)\n",
        "print(f\"‚úÖ Modelo salvo em: {model_path}\")\n",
        "\n",
        "# Salvar scaler se necess√°rio\n",
        "if model_uses_scaled:\n",
        "    scaler_path = '../scaler.pkl'\n",
        "    joblib.dump(scaler, scaler_path)\n",
        "    print(f\"‚úÖ Scaler salvo em: {scaler_path}\")\n",
        "\n",
        "# Salvar label encoders\n",
        "encoders_path = '../label_encoders.pkl'\n",
        "joblib.dump(label_encoders, encoders_path)\n",
        "print(f\"‚úÖ Label encoders salvos em: {encoders_path}\")\n",
        "\n",
        "print(f\"\\nüìù Informa√ß√µes do modelo salvo:\")\n",
        "print(f\"   - Modelo: {best_model_name}\")\n",
        "print(f\"   - Usa normaliza√ß√£o: {model_uses_scaled}\")\n",
        "print(f\"   - F1-Score: {best_model_score:.4f}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 9. Relat√≥rio de Classifica√ß√£o Detalhado\n",
        "\n",
        "Vamos ver o relat√≥rio completo de classifica√ß√£o do melhor modelo:\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Relat√≥rio de classifica√ß√£o do melhor modelo\n",
        "print(f\"üìä RELAT√ìRIO DE CLASSIFICA√á√ÉO - {best_model_name}\\n\")\n",
        "print(\"=\" * 70)\n",
        "\n",
        "if model_uses_scaled:\n",
        "    y_pred_best = best_model.predict(X_test_scaled)\n",
        "else:\n",
        "    y_pred_best = best_model.predict(X_test)\n",
        "\n",
        "print(classification_report(y_test, y_pred_best, \n",
        "                          target_names=['N√£o Evadiu', 'Evadiu']))\n",
        "print(\"=\" * 70)\n"
      ]
    }
  ],
  "metadata": {
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 2
}
